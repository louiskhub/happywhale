{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/louiskhub/happywhale/blob/main/model_trainign_louis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAp2ZwIgNgxj"
   },
   "source": [
    "Du brauchst die OurTrainingData zip im gleichen ordner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "5g3ISdSnN73R",
    "outputId": "088ef4fb-7b82-43cd-a1bc-9b29d0e87bcf"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 135\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuUtRRjFY3K1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIHizmmtORMw"
   },
   "outputs": [],
   "source": [
    "!rm -r OurTrainingData\n",
    "!rm OurTrainingData.zip\n",
    "!cp drive/MyDrive/OurTrainingData.zip OurTrainingData.zip\n",
    "!unzip OurTrainingData.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ForMw-mfJYk_"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"OurTrainingData/species_data.csv\", index_col=0)\n",
    "TARGET_SHAPE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_DATA_PATH = \"OurTrainingData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "f8sJ3zCBP2Ug"
   },
   "outputs": [],
   "source": [
    "def smart_batches(df: pd.core.frame.DataFrame, BATCH_SIZE: int, task: str = \"individual\",seed = 42, create_val_df = False) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    This is one of the most important functions:\n",
    "    -----------------\n",
    "    arguments:\n",
    "    df - pandas data frame of our data\n",
    "    seed - to generate same train/val split when reloading model\n",
    "    BATCH_SIZE - the bath_sie of our tensorflow dataset, must be even\n",
    "    task - either \"individual_id\" or \"species\", Specifies if we want to create train to identify species or individuals.\n",
    "    create_val_df - whether you want some indiviudals to be split up vor validation purposes -> only implemented when task=indivual\n",
    "    \n",
    "    -----------------\n",
    "    returns\n",
    "    Ordered Data Frame for Tensorflow Data set creation, such that the batches are valid for the triplet loss,\n",
    "    i.e. never contains only one positve.\n",
    "    \"\"\"\n",
    "    assert task in [\"individual\",\n",
    "                    \"species\"], 'task has to be either \"individual_id\" or \"species\"\" and must be column index of df'\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    if create_val_df:\n",
    "        assert task == \"individual\", \"only implemented when task=indivual\"\n",
    "\n",
    "    if task == \"individual\":\n",
    "        label = \"label\"\n",
    "        counts_column = \"individum_count\"\n",
    "        df = df_filter_for_indidum_training(df)\n",
    "    elif task == \"species\":\n",
    "        label = \"species_label\"\n",
    "        counts_column = \"species_counts\"\n",
    "    df = df.copy()\n",
    "    assert BATCH_SIZE % 2 == 0, \"BATCH_SIZE must be even\"\n",
    "\n",
    "    # refresh counts just in case\n",
    "    df[\"species_counts\"] = df.groupby('species_label')[\"species_label\"].transform('count')\n",
    "    df['individum_count'] = df.groupby('individual_id')['individual_id'].transform('count')\n",
    "\n",
    "    if create_val_df:\n",
    "        indexes_we_could_remove = list(df[df[\"individum_count\"] > 2].index)\n",
    "        random.shuffle(indexes_we_could_remove)\n",
    "\n",
    "        split_ratio = 0.05\n",
    "        cut = int(len(indexes_we_could_remove)*split_ratio)\n",
    "        keep_indexes = indexes_we_could_remove[cut:]\n",
    "        val_indexes = indexes_we_could_remove[:cut]\n",
    "        val_df = df.loc[val_indexes]\n",
    "        df = df.loc[keep_indexes]\n",
    "\n",
    "        # again, redo counts\n",
    "        df[\"species_counts\"] = df.groupby('species_label')[\"species_label\"].transform('count')\n",
    "        df['individum_count'] = df.groupby('individual_id')['individual_id'].transform('count')\n",
    "    else:\n",
    "        val_df = None\n",
    "\n",
    "    df[\"assign_to\"] = np.nan\n",
    "\n",
    "    even_mask = (df[counts_column] % 2 == 0).array\n",
    "    uneven_mask = np.logical_not(even_mask)\n",
    "\n",
    "    even_indices_list = list(df[even_mask].index)\n",
    "    uneven_df = df[uneven_mask]\n",
    "\n",
    "    amount_of_containers = math.ceil(len(df) / BATCH_SIZE)\n",
    "    container = np.array([BATCH_SIZE for i in range(amount_of_containers - 1)] + [len(df) % BATCH_SIZE])\n",
    "\n",
    "    set_of_uneven_classes = {a for a in uneven_df[label]}\n",
    "\n",
    "    if not len(set_of_uneven_classes) % 2 != container[-1] % 2:\n",
    "        unlucky_class = random.choice(uneven_df.index)\n",
    "        df.drop(index=unlucky_class)\n",
    "\n",
    "        even_mask = (df[counts_column] % 2 == 0).array\n",
    "        uneven_mask = np.logical_not(even_mask)\n",
    "\n",
    "        even_indices_list = list(df[even_mask].index)\n",
    "        uneven_df = df[uneven_mask]\n",
    "        set_of_uneven_classes = {a for a in uneven_df[label]}\n",
    "\n",
    "        print(f\"We threw away the datapoint with index {unlucky_class} \")\n",
    "\n",
    "    uneven_labels = {a: [] for a in uneven_df[label].array}\n",
    "    for index, int_label in zip(uneven_df.index, uneven_df[label].array):\n",
    "        uneven_labels[int_label].append(index)\n",
    "\n",
    "    for int_label in uneven_labels:\n",
    "        if len(uneven_labels[int_label]) > 3:\n",
    "            rest = uneven_labels[int_label][3:]\n",
    "            keep = uneven_labels[int_label][:3]\n",
    "            even_indices_list.extend(rest)\n",
    "\n",
    "            uneven_labels[int_label] = keep\n",
    "\n",
    "    uneven_indices_list = [uneven_labels[a] for a in uneven_labels]\n",
    "    random.shuffle(uneven_indices_list)\n",
    "\n",
    "    if len(set_of_uneven_classes) % 2 == 1:\n",
    "        container[-1] -= 3\n",
    "        first_triplet = uneven_indices_list.pop()\n",
    "        df.loc[first_triplet, \"assign_to\"] = len(container) - 1\n",
    "    assert len(uneven_indices_list) % 2 == 0, \"stf went horbly wrong\"\n",
    "\n",
    "    combined_double_triplets = [a + b for a, b in zip(uneven_indices_list[::2], uneven_indices_list[1::2])]\n",
    "    assert all([len(a) == 6 for a in combined_double_triplets])\n",
    "\n",
    "    even_df = df.loc[even_indices_list]\n",
    "    even_labels = even_df[label].sort_values().index\n",
    "\n",
    "    combined_even_doubles = [[a, b] for a, b in zip(even_labels[::2], even_labels[1::2])]\n",
    "    random.shuffle(combined_even_doubles)\n",
    "\n",
    "    assert all([df.loc[a, label] == df.loc[b, label] for a, b in combined_even_doubles])\n",
    "    i = 0\n",
    "    while combined_double_triplets:\n",
    "\n",
    "        if container[i] < 6:\n",
    "            i = i + 1 if i + 1 != len(container) else 0\n",
    "            continue\n",
    "\n",
    "        triplets = combined_double_triplets.pop()\n",
    "        container[i] -= 6\n",
    "        df.loc[triplets, \"assign_to\"] = i\n",
    "\n",
    "        i = i + 1 if i + 1 != len(container) else 0\n",
    "\n",
    "    i = 0\n",
    "    while combined_even_doubles:\n",
    "\n",
    "        if container[i] < 2:\n",
    "            i = i + 1 if i + 1 != len(container) else 0\n",
    "            continue\n",
    "\n",
    "        double = combined_even_doubles.pop()\n",
    "        container[i] -= 2\n",
    "        df.loc[double, \"assign_to\"] = i\n",
    "\n",
    "        i = i + 1 if i + 1 != len(container) else 0\n",
    "\n",
    "    assert np.all(container == 0)\n",
    "\n",
    "    return df.sort_values([\"assign_to\"]),val_df\n",
    "\n",
    "\n",
    "\n",
    "class DataSet_Generator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def prepare_images_mapping(self, path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.io.decode_jpeg(img, channels=3)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img *= (2 / 255)\n",
    "        img -= 1\n",
    "        return img, label\n",
    "\n",
    "    def augment(self, img, label):\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.70, 1.30)\n",
    "        img = tf.image.random_contrast(img, 0.80, 1.20)\n",
    "        img = tf.image.random_brightness(img, 0.10)\n",
    "        return img, label\n",
    "\n",
    "    def generate_species_data(self, df, factor_of_validation_ds=0.1, batch_size=None, augment=False,seed=None):\n",
    "        global TARGET_SHAPE\n",
    "        \"\"\"This function creates the tensorflow dataset for training:\n",
    "        -----------------\n",
    "        arguments:\n",
    "        df - pd.dataframe / Pandas dataframe containing the information for training\n",
    "        seed - to generate same train/val split when reloading model\n",
    "        factor_of_validation_ds - float / between 0 and 1 -> Percentage auf validation dataset for splitup.\n",
    "            Note: If we split increase the ds size via augmentation, the percentage will only be of the \"real\" data\n",
    "\n",
    "        batch_size - None,int / Batch-size for ds. If none specified -> take the one from utils.py\n",
    "        \n",
    "        augment - Bool/ wether you want to apply data augmentaion\n",
    "        -----------------\n",
    "        returns:\n",
    "        train_ds,val_ds\n",
    "        \"\"\"\n",
    "\n",
    "        # Asserts for function\n",
    "\n",
    "        assert 0 <= factor_of_validation_ds <= 1, \"Must be percentage\"\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = BATCH_SIZE  # if no batch size specified, we take the one from utils.py\n",
    "            print(f\"Since none Batch-size was specified we, took the {batch_size} specified in utils.py\")\n",
    "\n",
    "        df, _ = smart_batches(df, batch_size, task=\"species\",seed=seed)\n",
    "\n",
    "\n",
    "        image_paths = TRAIN_DATA_PATH + \"/\" + df[\"image\"]\n",
    "\n",
    "        number_of_classes = len(set(df[\"species_label\"]))\n",
    "\n",
    "        image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "        labels = tf.convert_to_tensor(df[\"species_label\"], dtype=tf.int32)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "        # map preprosessing\n",
    "        ds = ds.map(self.prepare_images_mapping , num_parallel_calls=8)\n",
    "\n",
    "        #one_hot encode labels\n",
    "        ds = ds.map(lambda img,label : (img, tf.one_hot(label, number_of_classes)))\n",
    "        if factor_of_validation_ds > 0:\n",
    "            length = math.floor(factor_of_validation_ds * len(ds))\n",
    "            val_ds = ds.take(length)\n",
    "            train_ds = ds.skip(length)\n",
    "        else:\n",
    "            val_ds = None\n",
    "            train_ds = ds\n",
    "            print(\"No validation set wanted, hence we will return None\")\n",
    "\n",
    "        if augment:\n",
    "            train_ds = train_ds.map(self.augment, num_parallel_calls=8)\n",
    "\n",
    "        train_ds = train_ds.batch(batch_size).prefetch(10)\n",
    "        val_ds = val_ds.batch(batch_size).prefetch(10)\n",
    "\n",
    "        ds = ds.batch(batch_size)\n",
    "\n",
    "        return train_ds,val_ds\n",
    "\n",
    "    def generate_individual_data(self, df, increase_ds_factor=1,batch_size=None,with_val_ds=False,seed=None):\n",
    "\n",
    "        global TARGET_SHAPE\n",
    "        \"\"\"This function creates the tensorflow dataset for training:\n",
    "        -----------------\n",
    "        arguments:\n",
    "        df - pd.dataframe / Pandas dataframe containing the information for training\n",
    "        seed - to generate same train/val split when reloading model\n",
    "        increase_ds_factor - int / either 1,2,3 -> By with factor do you want to increase dataset via augmentaion\n",
    "            1 -> keep size, no change\n",
    "            2 -> double ds size via augment1 function\n",
    "            3 -> triple ds size via augment1 + augment2 function\n",
    "\n",
    "        batch_size - None,int / Batch-size for ds. If none specified -> take the one from utils.py\n",
    "        with_val_ds - Split apart a small ds for accuracy estimations\n",
    "        -----------------\n",
    "        returns:\n",
    "        train_ds,val_ds\n",
    "        \"\"\"\n",
    "\n",
    "        # Asserts for function\n",
    "\n",
    "        assert increase_ds_factor in [1, 2, 3], \"Not supported value\"\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = BATCH_SIZE  # if no batch size specified, we take the one from utils.py\n",
    "            print(f\"Since none Batch-size was specified we, took the {batch_size} specified in utils.py\")\n",
    "\n",
    "        # Create order for the batches\n",
    "        df, val_df = smart_batches(df, batch_size, task=\"individuals\",create_val_df=with_val_ds,seed=seed,)\n",
    "\n",
    "        image_paths = TRAIN_DATA_PATH + \"/\" + df[\"image\"]\n",
    "\n",
    "        image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "\n",
    "        labels = tf.convert_to_tensor(df[\"label\"], dtype=tf.int32)\n",
    "\n",
    "        ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "\n",
    "        # map preprosessing\n",
    "        ds = ds.map(self.prepare_images_mapping, num_parallel_calls=8)\n",
    "        ds = ds.batch(batch_size)\n",
    "\n",
    "        if val_df is not None:\n",
    "            image_paths = TRAIN_DATA_PATH + \"/\" + val_df[\"image\"]\n",
    "            image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "            labels = tf.convert_to_tensor(val_df[\"label\"], dtype=tf.int32)\n",
    "            val_ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "            val_ds = val_ds.map(self.prepare_images_mapping, num_parallel_calls=8)\n",
    "        else:\n",
    "            val_ds = None\n",
    "\n",
    "        #if increase_ds_factor == 1:\n",
    "        #    pass\n",
    "        #elif increase_ds_factor == 2:\n",
    "        #    augmented_ds1 = ds.map(self.augment1, num_parallel_calls=8)\n",
    "        #    train_ds = ds.concatenate(augmented_ds1)\n",
    "        #elif increase_ds_factor == 3:\n",
    "        #    augmented_ds1 = ds.map(self.augment1, num_parallel_calls=8)\n",
    "        #    augmented_ds2 = ds.map(self.augment2, num_parallel_calls=8)#\n",
    "#\n",
    " #           train_ds = train_ds.concatenate(augmented_ds1)\n",
    "#          train_ds = train_ds.concatenate(augmented_ds2)\n",
    "\n",
    "        # Finally, batch train_ds\n",
    "        return ds,val_ds\n",
    "\n",
    "    # for now code leichen aber vielleicht spÃ¤ter\n",
    "    def augment1(self, x, label):\n",
    "        x = tf.image.random_crop(x, TARGET_SHAPE + (1,))\n",
    "        x = tf.image.random_flip_up_down(x)\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        return x, label\n",
    "\n",
    "    def augment2(self, x, label):\n",
    "        x = tf.image.random_contrast(x, 0.2, 0.5)\n",
    "        x = tf.image.random_brightness(x, 0.2)\n",
    "        x = tf.image.random_flip_up_down(x)\n",
    "        x = tf.image.random_flip_left_right(x)\n",
    "        return x, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUPgDirRQthZ",
    "outputId": "34772c8e-8caa-43be-df16-111094d6c051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We threw away the datapoint with index 24606 \n"
     ]
    }
   ],
   "source": [
    "train_ds,val_ds = DataSet_Generator().generate_species_data(df,batch_size=BATCH_SIZE, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8ft6I1mPkxS"
   },
   "outputs": [],
   "source": [
    "num_classes = len(set(df[\"species\"]))\n",
    "\n",
    "\n",
    "Input = tf.keras.Input((224,224,3))\n",
    "base = tf.keras.applications.densenet.DenseNet121(weights=\"imagenet\", include_top=False,input_tensor=Input)\n",
    "flatten = base.output\n",
    "flatten = tf.keras.layers.Flatten()(flatten)\n",
    "head = tf.keras.layers.Dense(256, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001))(flatten)\n",
    "head = tf.keras.layers.Dense(128, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001))(head)\n",
    "head = tf.keras.layers.Dense(64, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.l2(0.001))(head)\n",
    "head = tf.keras.layers.Dense(num_classes,activation=\"softmax\")(head)\n",
    "model = tf.keras.Model(inputs=Input, outputs=head,name=\"Inception_v3_with_imagenet_weights_and_l2\")\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=\"categorical_crossentropy\",metrics=[\"acc\"])\n",
    "\n",
    "if model.name not in os.listdir():\n",
    "    os.makedirs(model.name)\n",
    "    os.makedirs(model.name+\"/logs\")\n",
    "    os.makedirs(model.name+\"/saves\")\n",
    "    \n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_dir = model.name+\"/logs/\" +time_stamp \n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint_path = model.name+\"/saves/\" +time_stamp +\"/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQHr8BAGSmBS",
    "outputId": "13b7c360-6d87-4ab6-c2a1-5a8ee823117a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "  72/1436 [>.............................] - ETA: 17:03 - loss: 4.6540 - acc: 0.3116"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "train_ds,\n",
    "epochs=15,\n",
    "validation_data=val_ds,\n",
    "callbacks=[cp_callback,tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "model_trainign_louis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
