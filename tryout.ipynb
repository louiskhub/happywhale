{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cba33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from util import TRAIN_SPECIES_DF,SPECIES_SEED ,NUMBER_OF_SPECIES\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48af6b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>individum_count</th>\n",
       "      <th>label</th>\n",
       "      <th>species_label</th>\n",
       "      <th>species_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>1</td>\n",
       "      <td>12348</td>\n",
       "      <td>18</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16738</th>\n",
       "      <td>546ddeb9e61698.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>7c3b38f5ba08</td>\n",
       "      <td>1</td>\n",
       "      <td>7621</td>\n",
       "      <td>14</td>\n",
       "      <td>7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16742</th>\n",
       "      <td>5471e8805a8aef.jpg</td>\n",
       "      <td>dusky_dolphin</td>\n",
       "      <td>a001b65ffdfd</td>\n",
       "      <td>1</td>\n",
       "      <td>9769</td>\n",
       "      <td>8</td>\n",
       "      <td>3139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16748</th>\n",
       "      <td>547ee6a4582a34.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>52b9a021286d</td>\n",
       "      <td>1</td>\n",
       "      <td>5030</td>\n",
       "      <td>18</td>\n",
       "      <td>1689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16752</th>\n",
       "      <td>548462d50b56e4.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>e5dad0d8b4f5</td>\n",
       "      <td>1</td>\n",
       "      <td>14018</td>\n",
       "      <td>14</td>\n",
       "      <td>7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11462</th>\n",
       "      <td>3a09e969475d45.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>37c7aba965a5</td>\n",
       "      <td>400</td>\n",
       "      <td>3398</td>\n",
       "      <td>19</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10642</th>\n",
       "      <td>3598742ffdf746.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>37c7aba965a5</td>\n",
       "      <td>400</td>\n",
       "      <td>3398</td>\n",
       "      <td>19</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>098c074cef6ffd.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>37c7aba965a5</td>\n",
       "      <td>400</td>\n",
       "      <td>3398</td>\n",
       "      <td>19</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35299</th>\n",
       "      <td>b1406b6b80d9f4.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>37c7aba965a5</td>\n",
       "      <td>400</td>\n",
       "      <td>3398</td>\n",
       "      <td>19</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9398</th>\n",
       "      <td>2f41e02dffceae.jpg</td>\n",
       "      <td>minke_whale</td>\n",
       "      <td>37c7aba965a5</td>\n",
       "      <td>400</td>\n",
       "      <td>3398</td>\n",
       "      <td>19</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51033 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image             species individual_id  individum_count  \\\n",
       "0      00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9                1   \n",
       "16738  546ddeb9e61698.jpg      humpback_whale  7c3b38f5ba08                1   \n",
       "16742  5471e8805a8aef.jpg       dusky_dolphin  a001b65ffdfd                1   \n",
       "16748  547ee6a4582a34.jpg  melon_headed_whale  52b9a021286d                1   \n",
       "16752  548462d50b56e4.jpg      humpback_whale  e5dad0d8b4f5                1   \n",
       "...                   ...                 ...           ...              ...   \n",
       "11462  3a09e969475d45.jpg         minke_whale  37c7aba965a5              400   \n",
       "10642  3598742ffdf746.jpg         minke_whale  37c7aba965a5              400   \n",
       "1871   098c074cef6ffd.jpg         minke_whale  37c7aba965a5              400   \n",
       "35299  b1406b6b80d9f4.jpg         minke_whale  37c7aba965a5              400   \n",
       "9398   2f41e02dffceae.jpg         minke_whale  37c7aba965a5              400   \n",
       "\n",
       "       label  species_label  species_counts  \n",
       "0      12348             18            1689  \n",
       "16738   7621             14            7392  \n",
       "16742   9769              8            3139  \n",
       "16748   5030             18            1689  \n",
       "16752  14018             14            7392  \n",
       "...      ...            ...             ...  \n",
       "11462   3398             19            1608  \n",
       "10642   3398             19            1608  \n",
       "1871    3398             19            1608  \n",
       "35299   3398             19            1608  \n",
       "9398    3398             19            1608  \n",
       "\n",
       "[51033 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SPECIES_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b64fca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SPECIES= len(TRAIN_SPECIES_DF[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77188a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a Tensorflow dataset and Triplets for the Triplet loss function.\n",
    "\n",
    "Louis Kapp, Felix Hammer, Yannik Ullrich\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from util import TRAIN_DATA_PATH, BATCH_SIZE, TARGET_SHAPE\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "def redo_counts(df):\n",
    "    df = df.copy()\n",
    "    df[\"species_counts\"] = df.groupby('species_label')[\"species_label\"].transform('count')\n",
    "    df['individum_count'] = df.groupby('individual_id')['individual_id'].transform('count')\n",
    "    return df\n",
    "\n",
    "\n",
    "def triplet_loss_val_split(df, split_ratio, seed):\n",
    "    # We only want to take away individums with more then 2 images,so we still can use them for triplet-loss training\n",
    "\n",
    "    if split_ratio:\n",
    "        values_with_more_then_2_instances_df = df[df[\"individum_count\"] > 2]\n",
    "        values_with_2_instances_df = df[df[\"individum_count\"] == 2]\n",
    "\n",
    "        classes_we_could_remove = {i: [] for i in values_with_more_then_2_instances_df['label']}\n",
    "        indexes_we_cant_remove = list(values_with_2_instances_df.index)\n",
    "\n",
    "        for index in values_with_more_then_2_instances_df.index:\n",
    "            name = values_with_more_then_2_instances_df.loc[index, 'label']\n",
    "            classes_we_could_remove[name].append(index)\n",
    "\n",
    "        indexes_we_could_remove = list()\n",
    "\n",
    "        for name in classes_we_could_remove:\n",
    "            to_keep = classes_we_could_remove[name][:2]\n",
    "            to_remove = classes_we_could_remove[name][2:]\n",
    "\n",
    "            indexes_we_cant_remove.extend(to_keep)\n",
    "            indexes_we_could_remove.extend(to_remove)\n",
    "\n",
    "        # for every class throw away two training data\n",
    "\n",
    "        # seed for replicability\n",
    "        random.seed(seed)\n",
    "        # shuffle indexes for randomness\n",
    "        random.shuffle(indexes_we_could_remove)\n",
    "\n",
    "        # get cut length\n",
    "        cut = math.ceil(len(indexes_we_could_remove) * split_ratio)\n",
    "        # indexes we want to keep\n",
    "        keep_indexes = indexes_we_could_remove[cut:] + indexes_we_cant_remove\n",
    "        # indexes for val ds\n",
    "        val_indexes = indexes_we_could_remove[:cut]\n",
    "\n",
    "        # index dfs with chosen indexes\n",
    "        val_df = df.loc[val_indexes]\n",
    "        train_df = df.loc[keep_indexes]\n",
    "\n",
    "        # redo counts\n",
    "        val_df = redo_counts(val_df)\n",
    "        train_df = redo_counts(train_df)\n",
    "\n",
    "        return train_df, val_df\n",
    "    else:\n",
    "        return df, None\n",
    "\n",
    "\n",
    "def split_df_by_even_uneven(train_df, counts_column, label):\n",
    "    # redo counts\n",
    "    train_df = redo_counts(train_df)\n",
    "    # split\n",
    "    even_df = train_df[train_df[counts_column] % 2 == 0]\n",
    "    uneven_df = train_df[train_df[counts_column] % 2 == 1]\n",
    "    # get the indexes of the data-points with even/ uneven occurrences\n",
    "    even_indices_list = list(even_df.index)\n",
    "    # get the set of uneven classes\n",
    "    set_of_uneven_classes = {a for a in uneven_df[label]}\n",
    "    return even_df, uneven_df, even_indices_list, set_of_uneven_classes\n",
    "\n",
    "\n",
    "def shuffle_container_order(train_df, amount_of_containers):\n",
    "    \"\"\" This part is primarily for a good train,test,val split for our species data, in such a way that every dataset contains instances of all species.\n",
    "    To achieve this more often we shuffle the order of containers:\"\"\"\n",
    "\n",
    "    train_df = train_df.copy()\n",
    "\n",
    "    last_container = amount_of_containers  # We do not want to shuffle the last one\n",
    "    list_to_shuffle = list(range(0, amount_of_containers - 1))  # hence, the minus 1\n",
    "    random.shuffle(list_to_shuffle)\n",
    "    list_to_shuffle.append(last_container - 1)  # append the last container\n",
    "\n",
    "    # reassign order\n",
    "    train_df[\"assign_to\"] = np.array([list_to_shuffle[int(i - 1)] for i in train_df[\"assign_to\"]])\n",
    "    return train_df\n",
    "\n",
    "\n",
    "is_even = lambda x: x % 2 == 0\n",
    "\n",
    "\n",
    "def smart_batches(df: pd.core.frame.DataFrame, BATCH_SIZE: int, task: str = \"individual\", seed=0,\n",
    "                  val_split=0.1) -> pd.core.frame.DataFrame:\n",
    "    \"\"\"\n",
    "    This is one of the most important functions:\n",
    "    -----------------\n",
    "    arguments:\n",
    "    df - pandas data frame of our data\n",
    "    seed - to generate same train/val split when reloading model\n",
    "    BATCH_SIZE - the bath_sie of our tensorflow dataset, must be even\n",
    "    task - either \"individual_id\" or \"species\", Specifies if we want to create train to identify species or individuals.\n",
    "    val_split - whether you want some indiviudals to be split up vor validation purposes -> only implemented when task=indivual\n",
    "    \n",
    "    -----------------\n",
    "    returns\n",
    "    Ordered Data Frame for Tensorflow Data set creation, such that the batches are valid for the triplet loss,\n",
    "    i.e. never contains only one positve.\n",
    "    \"\"\"\n",
    "    assert task in [\"individual\",\n",
    "                    \"species\"], 'task has to be either \"individual_id\" or \"species\"\" and must be column index of df'\n",
    "\n",
    "    assert is_even(BATCH_SIZE), \"BATCH_SIZE must be even\"\n",
    "\n",
    "    # refresh counts just in case\n",
    "    df = redo_counts(df)\n",
    "\n",
    "    if task == \"individual\":\n",
    "        label = \"label\"\n",
    "        counts_column = \"individum_count\"\n",
    "        df = df[df[\"individum_count\"] > 1]\n",
    "        # generate split\n",
    "        train_df, val_df = triplet_loss_val_split(df, val_split, seed)\n",
    "\n",
    "    elif task == \"species\":\n",
    "        label = \"species_label\"\n",
    "        counts_column = \"species_counts\"\n",
    "        train_df = df\n",
    "\n",
    "    # now we start working on the constraint problem\n",
    "    # first we need to know the amount of containers\n",
    "    amount_of_containers = math.ceil(len(train_df) / BATCH_SIZE)\n",
    "    # then we make a numpy array, which holds for every container the amount of space\n",
    "    container = np.zeros(amount_of_containers)\n",
    "    container[:-1] = BATCH_SIZE\n",
    "    # the last container has just the amount of data-points which are missing\n",
    "\n",
    "    container[-1] = len(train_df) % BATCH_SIZE\n",
    "    # assert that the last container has at least 2 places\n",
    "    assert container[-1] >= 2, \"A very unlikely case happened, try a val_split which is just a bit different or in \" \\\n",
    "                               \"case of species remove 2 random data-points from your df \"\n",
    "\n",
    "    # new column container assignment\n",
    "    train_df[\"assign_to\"] = np.nan\n",
    "\n",
    "    # get dfs containing even/uneven values + indices of even datapoints + the set of uneven classes\n",
    "    even_df, uneven_df, even_indices_list, set_of_uneven_classes = split_df_by_even_uneven(train_df, counts_column,\n",
    "                                                                                           label)\n",
    "\n",
    "    # make a dict with uneven_labels as keys and a empty list as value\n",
    "    uneven_labels = {a: [] for a in uneven_df[label].array}\n",
    "\n",
    "    # then assign every key all the indexes of the data-points which belong to it\n",
    "    for index, int_label in zip(uneven_df.index, uneven_df[label].array):\n",
    "        uneven_labels[int_label].append(index)\n",
    "\n",
    "    # now we want to have only pairs of 3 datapoint for each uneven label\n",
    "    # to achieve this we simply keep 3 and put the rest, which we know are always and even amount to the even indexes\n",
    "    for int_label in uneven_labels:\n",
    "        # if we have more then 3 data-points\n",
    "        if len(uneven_labels[int_label]) > 3:\n",
    "            # triplet we want to keep\n",
    "            keep = uneven_labels[int_label][:3]\n",
    "            # rest we want to put to the evenset\n",
    "            rest = uneven_labels[int_label][3:]\n",
    "            # put it there\n",
    "            even_indices_list.extend(rest)\n",
    "            # keep only triplet\n",
    "            uneven_labels[int_label] = keep\n",
    "\n",
    "    # now we create the list of uneven indices and shuffle it for randomness\n",
    "    uneven_indices_list = [uneven_labels[a] for a in uneven_labels]\n",
    "    random.shuffle(uneven_indices_list)\n",
    "\n",
    "    # Now we have a delicate last problem\n",
    "    # if we have an even amount of uneven classes and an even amount of space in the last container we do not have a problem\n",
    "    if is_even(len(uneven_indices_list)) and is_even(container[-1]):\n",
    "        pass\n",
    "\n",
    "    # If we have an uneven amount of uneven classes and even amount of space in the last container\n",
    "    # we just put a triplet in to the last container to make everything even\n",
    "    elif not is_even(len(uneven_indices_list)) and not is_even(container[-1]):\n",
    "        # we put 3 items in -> 3 space less\n",
    "        container[-1] -= 3\n",
    "        # get the first triplet\n",
    "        first_triplet = uneven_indices_list.pop()\n",
    "        # assign it to last container (we start counting with 0 like in a true pythonic fashion, hence the -1)\n",
    "        train_df.loc[first_triplet, \"assign_to\"] = len(container) - 1\n",
    "\n",
    "    # This case should never happen, if so something strange happened\n",
    "    else:\n",
    "        raise NameError('We made an error in the concept of the algorithm')\n",
    "\n",
    "\n",
    "    # Now we should have only an even amount of triplet pairs\n",
    "    assert is_even(len(uneven_indices_list)), \"stf went horribly wrong\"\n",
    "    assert is_even(len(even_indices_list)), \"stf went horribly wrong\"\n",
    "\n",
    "    # because it is even we now can generate the pairs 3+3 = 6 nicely by zipping + clever indexing\n",
    "    combined_double_triplets = [a + b for a, b in zip(uneven_indices_list[::2], uneven_indices_list[1::2])]\n",
    "\n",
    "\n",
    "    assert all([len(a) == 6 for a in combined_double_triplets])\n",
    "\n",
    "    # beacause we assigned the some members of the uneven set to the even_indices_list we have to redo our even_df\n",
    "    even_df = train_df.loc[even_indices_list]\n",
    "\n",
    "    # no we want to form the double pairs\n",
    "    # to to this we sort by label to then apply clever indexing (we can do this because we know that every label is represented an even amount of times in the df\n",
    "    even_labels = even_df[label].sort_values().index\n",
    "    # create pairs by indexing + zipping\n",
    "    combined_even_doubles = [[a, b] for a, b in zip(even_labels[::2], even_labels[1::2])]\n",
    "    # shuffle again for randomness\n",
    "    random.shuffle(combined_even_doubles)\n",
    "\n",
    "    # We check whether a tuple has the same label\n",
    "    assert all([train_df.loc[a, label] == train_df.loc[b, label] for a, b in combined_even_doubles])\n",
    "\n",
    "    # No all the hard work is done, and it is time to distribute our data-points to the container\n",
    "\n",
    "    # small func to make the next part beautifuler\n",
    "    next_step = lambda i: i + 1 if i + 1 != len(container) else 0\n",
    "\n",
    "    # distribute the uneven triplets\n",
    "    i = 0\n",
    "    while combined_double_triplets:\n",
    "\n",
    "        # if the container does not have enough space-> go to next\n",
    "        if container[i] < 6:\n",
    "            i = next_step(i)\n",
    "            continue\n",
    "        # get the first triplet pair\n",
    "        triplets = combined_double_triplets.pop()\n",
    "        # assign it\n",
    "        train_df.loc[triplets, \"assign_to\"] = i\n",
    "        # container has less space now\n",
    "        container[i] -= 6\n",
    "\n",
    "        i = next_step(i)\n",
    "\n",
    "    # distribute the even triplets\n",
    "    i = 0\n",
    "    while combined_even_doubles:\n",
    "        # if the container does not have enough space-> go to next\n",
    "        if container[i] < 2:\n",
    "            i = i + 1 if i + 1 != len(container) else 0\n",
    "            continue\n",
    "\n",
    "        double = combined_even_doubles.pop()\n",
    "        container[i] -= 2\n",
    "        train_df.loc[double, \"assign_to\"] = i\n",
    "\n",
    "        i = i + 1 if i + 1 != len(container) else 0\n",
    "\n",
    "    # all containers should be empty now\n",
    "    assert np.all(container == 0)\n",
    "\n",
    "    # This part is primarily for a good train,test,val split for our species data, in such a way that every dataset contains instances of all species\n",
    "    # We shuffle the order of containers\n",
    "    train_df = shuffle_container_order(train_df, amount_of_containers)\n",
    "\n",
    "    # By sorting by the assignment-order we now achieve the good ordering for correct batches\n",
    "    train_df = train_df.sort_values([\"assign_to\"])\n",
    "\n",
    "    if task == \"individual\":\n",
    "        return train_df, val_df\n",
    "    elif task == \"species\":\n",
    "        return train_df\n",
    "\n",
    "\n",
    "class DS_Generator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def build_ds(self, imgage_paths, classes):\n",
    "        image_paths = TRAIN_DATA_PATH + \"/\" + imgage_paths\n",
    "        image_paths = tf.convert_to_tensor(image_paths, dtype=tf.string)\n",
    "        labels = tf.convert_to_tensor(classes, dtype=tf.int32)\n",
    "        ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        ds = ds.map(self.prepare_images_mapping, num_parallel_calls=8)\n",
    "        return ds\n",
    "\n",
    "    def generate_single_individuals_ds(self,df,batch_size):\n",
    "        df = df[df[\"individum_count\"]==1]\n",
    "        return self.build_ds(df[\"image\"], df[\"label\"]).batch(batch_size).prefetch(10), df\n",
    "\n",
    "    def prepare_images_mapping(self, path, label):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.io.decode_jpeg(img, channels=3)\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        img *= (2 / 255)\n",
    "        img -= 1\n",
    "        return img, label\n",
    "\n",
    "    def augment(self, img, label):\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_hue(img, 0.01)\n",
    "        img = tf.image.random_saturation(img, 0.70, 1.30)\n",
    "        img = tf.image.random_contrast(img, 0.80, 1.20)\n",
    "        img = tf.image.random_brightness(img, 0.10)\n",
    "        return img, label\n",
    "\n",
    "    def generate_species_data(self, df, factor_of_validation_ds=0.1, factor_of_test_ds=0.1, batch_size=None,\n",
    "                              augment=False, seed=None, return_eval_data=False):\n",
    "        global NUMBER_OF_SPECIES\n",
    "        \"\"\"This function creates the tensorflow dataset for training:\n",
    "        -----------------\n",
    "        arguments:\n",
    "        df - pd.dataframe / Pandas dataframe containing the information for training\n",
    "        seed - to generate same train/val split when reloading model\n",
    "        factor_of_validation_ds - float / between 0 and 1 -> Percentage auf validation dataset for splitup.\n",
    "            Note: If we split increase the ds size via augmentation, the percentage will only be of the \"real\" data\n",
    "\n",
    "        \n",
    "        batch_size - None,int / Batch-size for ds. If none specified -> take the one from utils.py\n",
    "        \n",
    "        augment - Bool/ wether you want to apply data augmentaion\n",
    "        return_eval_data - whether to return data for evaluation (test_ds + df)\n",
    "        -----------------\n",
    "        returns:\n",
    "        train_ds,val_ds\n",
    "        \"\"\"\n",
    "\n",
    "        # Asserts for function\n",
    "\n",
    "        assert 0 <= factor_of_validation_ds <= 1, \"Must be percentage\"\n",
    "        assert 0 <= factor_of_test_ds <= 1, \"Must be percentage\"\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = BATCH_SIZE  # if no batch size specified, we take the one from utils.py\n",
    "            print(f\"Since none Batch-size was specified we, took the {batch_size} specified in utils.py\")\n",
    "\n",
    "        df = smart_batches(df, batch_size, task=\"species\", seed=seed)\n",
    "\n",
    "        ds = self.build_ds(df[\"image\"], df[\"label\"])\n",
    "\n",
    "        # one_hot encode labels\n",
    "        ds = ds.map(lambda img, label: (img, tf.one_hot(label, NUMBER_OF_SPECIES)))\n",
    "\n",
    "        df[\"which_set\"] = \"new_column\"\n",
    "\n",
    "        val_length = math.floor(factor_of_validation_ds * len(ds))\n",
    "        val_ds = ds.take(val_length)\n",
    "        df.iloc[range(val_length), -1] = \"val_ds\"\n",
    "\n",
    "        test_length = math.floor(factor_of_test_ds * len(ds))\n",
    "        test_ds = ds.take(test_length)\n",
    "        df.iloc[range(val_length, val_length + test_length), -1] = \"test_ds\"\n",
    "\n",
    "        train_ds = ds.skip(val_length + test_length)\n",
    "        df.iloc[range(val_length + test_length, len(ds)), -1] = \"train_ds\"\n",
    "\n",
    "        if augment:\n",
    "            train_ds = train_ds.map(self.augment, num_parallel_calls=8)\n",
    "\n",
    "        train_ds = train_ds.batch(batch_size).prefetch(10)\n",
    "        val_ds = val_ds.batch(batch_size).prefetch(10)\n",
    "\n",
    "        if not return_eval_data:\n",
    "            return train_ds, val_ds\n",
    "        elif return_eval_data:\n",
    "            return train_ds, val_ds, test_ds, df\n",
    "\n",
    "    def generate_individual_data(self, df, augment=False, batch_size=None, seed=None, val_split=0.1, return_eval_data=False):\n",
    "        \"\"\"This function creates the tensorflow dataset for training:\n",
    "        -----------------\n",
    "        arguments:\n",
    "        df - pd.dataframe / Pandas dataframe containing the information for training\n",
    "        seed - to generate same train/val split when reloading model\n",
    "        increase_ds_factor - int / either 1,2,3 -> By with factor do you want to increase dataset via augmentaion\n",
    "            1 -> keep size, no change\n",
    "            2 -> double ds size via augment1 function\n",
    "            3 -> triple ds size via augment1 + augment2 function\n",
    "\n",
    "        batch_size - None,int / Batch-size for ds. If none specified -> take the one from utils.py\n",
    "        with_val_ds - Split apart a small ds for accuracy estimations\n",
    "        -----------------\n",
    "        returns:\n",
    "        train_ds,val_ds\n",
    "        \"\"\"\n",
    "\n",
    "        # Asserts for function\n",
    "\n",
    "        if batch_size is None:\n",
    "            batch_size = BATCH_SIZE  # if no batch size specified, we take the one from utils.py\n",
    "            print(f\"Since none Batch-size was specified we, took the {batch_size} specified in utils.py\")\n",
    "\n",
    "        # Create order for the batches\n",
    "        train_df, val_df = smart_batches(df, batch_size, task=\"individual\", seed=seed, val_split=val_split)\n",
    "\n",
    "        train_ds = self.build_ds(train_df[\"image\"], train_df[\"label\"])\n",
    "        if augment:\n",
    "            train_ds = train_ds.map(self.augment)\n",
    "        train_ds = train_ds.batch(batch_size).prefetch(10)\n",
    "\n",
    "        val_ds = self.build_ds(val_df[\"image\"], val_df[\"label\"])\n",
    "        val_ds = val_ds.batch(batch_size).prefetch(10)\n",
    "\n",
    "        if return_eval_data == False:\n",
    "            return train_ds\n",
    "        elif return_eval_data:\n",
    "            return train_ds, val_ds, train_df, val_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8edd5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds = DS_Generator().generate_species_data(TRAIN_SPECIES_DF,augment=1,batch_size=64,seed=SPECIES_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b88d8609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"Saved Models/inception_v3_max_pooling_imagenet_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6acc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.Dense(NUMBER_OF_SPECIES,activation=\"softmax\",)(model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=model.input, outputs=output,name=\"SomeNiceName\")\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=\"categorical_crossentropy\",metrics=[\"acc\",tf.keras.metrics.TopKCategoricalAccuracy(\n",
    "    k=3, name='top_3_categorical_accuracy'\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a1483f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "638/638 [==============================] - ETA: 0s - loss: 8.3588 - acc: 0.0210 - top_3_categorical_accuracy: 0.0458\n",
      "Epoch 1: saving model to ../models/SomeNiceName/saves/20220402-214710\\cp-0001.ckpt\n",
      "638/638 [==============================] - 310s 462ms/step - loss: 8.3588 - acc: 0.0210 - top_3_categorical_accuracy: 0.0458 - val_loss: 8.4988 - val_acc: 0.0249 - val_top_3_categorical_accuracy: 0.0560\n",
      "Epoch 2/35\n",
      "638/638 [==============================] - ETA: 0s - loss: 6.9342 - acc: 0.0348 - top_3_categorical_accuracy: 0.0762\n",
      "Epoch 2: saving model to ../models/SomeNiceName/saves/20220402-214710\\cp-0002.ckpt\n",
      "638/638 [==============================] - 289s 449ms/step - loss: 6.9342 - acc: 0.0348 - top_3_categorical_accuracy: 0.0762 - val_loss: 76.7179 - val_acc: 0.0219 - val_top_3_categorical_accuracy: 0.0901\n",
      "Epoch 3/35\n",
      "638/638 [==============================] - ETA: 0s - loss: 5.9316 - acc: 0.0574 - top_3_categorical_accuracy: 0.1209\n",
      "Epoch 3: saving model to ../models/SomeNiceName/saves/20220402-214710\\cp-0003.ckpt\n",
      "638/638 [==============================] - 290s 452ms/step - loss: 5.9316 - acc: 0.0574 - top_3_categorical_accuracy: 0.1209 - val_loss: 26.6082 - val_acc: 0.0317 - val_top_3_categorical_accuracy: 0.0725\n",
      "Epoch 4/35\n",
      "293/638 [============>.................] - ETA: 2:24 - loss: 4.3698 - acc: 0.1552 - top_3_categorical_accuracy: 0.2946"
     ]
    }
   ],
   "source": [
    "# create folders\n",
    "if model.name not in os.listdir(\"../models/\"):\n",
    "    os.makedirs(\"../models/\"+model.name)\n",
    "    os.makedirs(\"../models/\"+model.name+\"/logs\")\n",
    "    os.makedirs(\"../models/\"+model.name+\"/saves\")\n",
    "\n",
    "# timestamp for logging\n",
    "time_stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_dir = \"../models/\"+model.name+\"/logs/\" +time_stamp\n",
    "\n",
    "# callback or performance\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "checkpoint_path = \"../models/\"+model.name+\"/saves/\" +time_stamp +\"/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "# callback or weights saving\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True)\n",
    "\n",
    "model.fit(\n",
    "train_ds,\n",
    "epochs=35,\n",
    "validation_data=val_ds,\n",
    "callbacks=[cp_callback,tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whale",
   "language": "python",
   "name": "whale"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
